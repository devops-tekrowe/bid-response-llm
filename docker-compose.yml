version: "3.9"

services:
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage

  qwen-vllm:
    image: vllm/vllm-openai:latest
    container_name: qwen-vllm
    restart: unless-stopped
    environment:
      # HF token is read from your shell env: export HF_TOKEN=...
      HF_TOKEN: "${HF_TOKEN}"
      HUGGING_FACE_HUB_TOKEN: "${HF_TOKEN}"
    ports:
      - "8000:8000"
    # GPU + IPC settings (approx. equivalent to: --runtime nvidia --ipc=host)
    runtime: nvidia
    ipc: host
    command: >
      --model Qwen/Qwen3-0.6B

  api:
    build:
      context: .
    container_name: rfq-api
    restart: unless-stopped
    env_file:
      - .env
    environment:
      # Override local Qdrant URL to point at the Docker service
      QDRANT_LOCAL_URL: http://qdrant:6333
      # Use the qwen-vllm service as the OpenAI-compatible LLM endpoint
      LOCAL_LLM_BASE_URL: http://qwen-vllm:8000/v1
    ports:
      - "9000:9000"
    depends_on:
      - qdrant
    working_dir: /app
    volumes:
      - .:/app
    command: >
      uvicorn rfq_api:app
      --host 0.0.0.0
      --port 9000

volumes:
  qdrant_storage:
    driver: local

